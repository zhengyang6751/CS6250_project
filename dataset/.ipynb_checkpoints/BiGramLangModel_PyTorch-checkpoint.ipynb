{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OEbaodEV9wx",
    "outputId": "9f3157b2-30c1-4933-c2b6-ab57d1ab5def"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/zhengyang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZeoMDjvnSkQ",
    "outputId": "73f1ad73-0b69-4a93-cff1-5b5012bf8701"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "{'resting', 'she', 'to', 'over', 'and', 'the', 'rushes', 'jumps', 'but', 'can', 'behind', 'past', 'dashes', 'sighs', 'he', 'yawns', 'lazy', 'while', 'fence', 'watches', ' ', 'brown', 'quickly', 'teases', 'bright', 'leaps', 'move', 'beside', 'leaving', 'clever', 'sleepy', 'higher', 'calm', 'slow', 'jumping', 'sleeps', 'already', 'because', 'through', 'under', 'ignores', 'remains', 'waits', 'small', 'until', 'chases', '.', 'is', 'a', 'before', 'sees', 'too', 'naps', 'dog', 'closes', 'running', 'stretches', 'moon', 'who', 'tree', 'barks', 'him', 'eyes', 'as', 'at', 'his', 'river', 'refuses', 'splashes', 'still', 'quick', 'runs', ',', 'than', 'playing', 'fox', 'grass', 'nearby', 'gone', 'see', 'circles'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, batch_size=4, input_length=8, train_iters=100, eval_iters=100):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        # input_length = how many consecutive tokens/chars in one input\n",
    "        self.input_length = input_length\n",
    "        # batch_size = how many inputs are going to be processed in-parallel (on GPU)\n",
    "        self.batch_size = batch_size\n",
    "        # train_iters = how many training iterations\n",
    "        self.train_iters= train_iters\n",
    "        # eval_iters = how many batches to evaluate to get average performance\n",
    "        self.eval_iters = eval_iters\n",
    "\n",
    "    def forward(self, inputs, targets=None):\n",
    "        logits = self.token_embeddings_table(inputs)\n",
    "        # print(logits.shape)\n",
    "        # logits are estimated model parameters\n",
    "        # for each input of context_size, there are vocab_size parameters to be estimated\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, input_length, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size * input_length, vocab_size)\n",
    "            targets = targets.view(batch_size * input_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def fit(self, learning_rate=0.001):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        for iter in range(self.train_iters):\n",
    "            if iter % (self.train_iters//20) == 0:\n",
    "                avg_loss = self.eval_loss()\n",
    "                print(f\"iter {iter} train {avg_loss['train']} val {avg_loss['eval']}\")\n",
    "            inputs, targets = self.get_batch(split='train')\n",
    "            logits, loss = self(inputs, targets)\n",
    "            optimizer.zero_grad(set_to_none=True)  # clear gradients of previous step\n",
    "            loss.backward()  # propagate loss back to the each unit in the network\n",
    "            optimizer.step()  # update network parameters w.r.t the loss\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "    def generate(self, context, max_new_tokens):\n",
    "        inputs = context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # forward pass, targets None, loss None\n",
    "            logits, _ = self(inputs)\n",
    "            # only last char/time-step is needed\n",
    "            logits = logits[:, -1, :]\n",
    "            # softmax logits to get probability distribution\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            # sample\n",
    "            sampled_output = torch.multinomial(probs, num_samples=1)\n",
    "            # append the sampled_output to running outputs\n",
    "            inputs = torch.cat((inputs, sampled_output), dim=1)\n",
    "        output_text = self.decoder(inputs[0].tolist())\n",
    "        return output_text\n",
    "\n",
    "    @torch.no_grad() # tell torch not to prepare for back-propagation\n",
    "    def eval_loss(self):\n",
    "        perf = {}\n",
    "        # set dropout and batch normalization layers to evaluation mode before running inference.\n",
    "        self.eval()\n",
    "        for split in ['train', 'eval']:\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                inputs, targets = self.get_batch(split)  # get random batch of inputs and targete\n",
    "                logits, loss = self(inputs, targets)  # forward pass\n",
    "                losses[k] = loss.item()  # the value of loss tensor as a standard Python number\n",
    "            perf[split] = losses.mean()\n",
    "        self.train() # turn-on training mode-\n",
    "        return perf\n",
    "\n",
    "    def prep(self, text):\n",
    "        vocab = sorted(list(set(text)))\n",
    "        self.vocab_size = len(vocab)\n",
    "        # look-up table for\n",
    "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
    "\n",
    "        ctoi = {c: i for i, c in enumerate(vocab)}  # char c to integer i map. assign value i for every word in vocab\n",
    "        itoc = {i: c for c, i in ctoi.items()}  # integer i to char c map\n",
    "\n",
    "        # print(ctoi)\n",
    "        # print(itoc)\n",
    "\n",
    "        self.encoder = lambda text: [ctoi[c] for c in text]\n",
    "        self.decoder = lambda nums: ''.join([itoc[i] for i in nums])\n",
    "\n",
    "        n = len(text)\n",
    "        self.train_text = text[:int(n * 0.9)]\n",
    "        self.val_text = text[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "    def prep_tokens(self, text):\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens.append(' ')\n",
    "        vocab = set(tokens)\n",
    "        print(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "        # look-up table for\n",
    "        self.token_embeddings_table = nn.Embedding(self.vocab_size, self.vocab_size)\n",
    "\n",
    "        ctoi = {c: i for i, c in enumerate(vocab)}  # token c to integer i map. assign value i for every word in vocab\n",
    "        itoc = {i: c for c, i in ctoi.items()}  # integer i to token c map\n",
    "\n",
    "        # print(ctoi)\n",
    "        # print(itoc)\n",
    "\n",
    "        self.encoder = lambda text: [ctoi[c] for c in tokens]\n",
    "        self.decoder = lambda nums: ' '.join([itoc[i] for i in nums])\n",
    "\n",
    "        n = len(tokens)\n",
    "        self.train_text = tokens[:int(n * 0.9)]\n",
    "        self.val_text = tokens[int(n * 0.9):]\n",
    "\n",
    "        self.train_data = torch.tensor(self.encoder(self.train_text), dtype=torch.long)\n",
    "        self.val_data = torch.tensor(self.encoder(self.val_text), dtype=torch.long)\n",
    "\n",
    "    def get_batch(self, split='train'):\n",
    "        data = self.train_data if split == 'train' else self.val_data\n",
    "        ix = torch.randint(len(data) - self.input_length,\n",
    "                           (self.batch_size,))  # get random chunks of length batch_size from data\n",
    "        inputs_batch = torch.stack([data[i:i + self.input_length] for i in ix])\n",
    "        targets_batch = torch.stack([data[i + 1:i + self.input_length + 1] for i in ix])\n",
    "        inputs_batch = inputs_batch.to(self.device)  # deploy to GPU is available\n",
    "        targets_batch = targets_batch.to(self.device)# deploy to GPU is available\n",
    "        return inputs_batch, targets_batch\n",
    "\n",
    "def fetch_text_from_url(url):\n",
    "    \"\"\"Fetches raw text from a given URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text\n",
    "\n",
    "def read_local_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# 2. 替换 fetch_text_from_url 部分\n",
    "file_path = \"WarrenBuffet.txt\"  # 替换为你的本地文件路径\n",
    "text = read_local_txt(file_path)\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "model = BigramLanguageModel(batch_size=32,\n",
    "                            input_length=8,\n",
    "                            train_iters=5000)\n",
    "model = model.to(model.device)\n",
    "model.prep_tokens(text)\n",
    "input_batch, output_batch = model.get_batch(split='train')\n",
    "# print(input_batch)\n",
    "# print(output_batch)\n",
    "\n",
    "logits, loss = model(input_batch, output_batch)\n",
    "# print(logits.shape)\n",
    "# print(logits)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9SC1I_hTdXC",
    "outputId": "7778cda9-d887-4869-9e9f-eab738e6325b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resting behind sleeps ignores jumping tree at barks stretches as over than quick clever teases   him teases lazy past watches behind playing and barks who jumping quick barks beside she he quickly because until sees calm dashes move beside sleeps a closes slow than quickly bright slow fence tree still chases eyes , chases past gone ignores refuses but him because a stretches resting too eyes can naps teases see tree eyes quick the quick barks clever at rushes resting and until eyes quick slow fox teases slow lazy can naps remains tree through jumping eyes runs sees he eyes\n",
      "Vocab size 81, CE: 4.394449154672439\n",
      "iter 0 train 4.829953670501709 val 4.820611476898193\n",
      "iter 250 train 1.2270034551620483 val 1.2359185218811035\n",
      "iter 500 train 1.2307583093643188 val 1.226365089416504\n",
      "iter 750 train 1.2268770933151245 val 1.2256004810333252\n",
      "iter 1000 train 1.2318692207336426 val 1.2319931983947754\n",
      "iter 1250 train 1.2256922721862793 val 1.2155133485794067\n",
      "iter 1500 train 1.2293990850448608 val 1.2206721305847168\n",
      "iter 1750 train 1.2278296947479248 val 1.2123076915740967\n",
      "iter 2000 train 1.2225637435913086 val 1.2217588424682617\n",
      "iter 2250 train 1.2208738327026367 val 1.21734619140625\n",
      "iter 2500 train 1.2238725423812866 val 1.2202250957489014\n",
      "iter 2750 train 1.2301336526870728 val 1.21430242061615\n",
      "iter 3000 train 1.231619954109192 val 1.2160451412200928\n",
      "iter 3250 train 1.2257122993469238 val 1.219932198524475\n",
      "iter 3500 train 1.2285960912704468 val 1.2238246202468872\n",
      "iter 3750 train 1.2261871099472046 val 1.2307544946670532\n",
      "iter 4000 train 1.226442813873291 val 1.2324963808059692\n",
      "iter 4250 train 1.2208091020584106 val 1.2317124605178833\n",
      "iter 4500 train 1.2260589599609375 val 1.2243788242340088\n",
      "iter 4750 train 1.2326819896697998 val 1.2316253185272217\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "outputs = model.generate(context=torch.zeros((1, 1), dtype=torch.long, device=model.device),\n",
    "                         max_new_tokens=100)\n",
    "print(outputs)\n",
    "print(f\"Vocab size {model.vocab_size}, CE: {-np.log(1/model.vocab_size)}\")\n",
    "model.fit(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aq-N3SQ32U1q",
    "outputId": "c0b09391-102e-43c7-a5ff-68adf4d3ba24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he is brown fox rushes past . the fox watches the dog naps as the brown fox is already gone . dog can see . the dog ignores the grass , but he is lazy and the fox waits until the sleepy dog is lazy dog is too slow . the fox waits . the fox waits until the brown fox sees the dog because he is too slow . she quickly . beside the grass , leaving the brown . a quick brown fox jumps over the fox rushes past . the sleepy dog barks at the grass , but\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(context=torch.zeros((1, 1), dtype=torch.long, device=model.device), max_new_tokens=100)\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
