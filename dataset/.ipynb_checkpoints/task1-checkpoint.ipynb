{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa6cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.util import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import math\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "class BigramLanguageModel:\n",
    "    def __init__(self, lambda_smoothing=0.1):\n",
    "        self.unigram_counts = Counter()\n",
    "        self.bigram_counts = Counter()\n",
    "        self.vocab_size = 0\n",
    "        self.lambda_smoothing = lambda_smoothing\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Cleans and tokenizes text.\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Normalize spaces\n",
    "        #text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Remove punctuation\n",
    "        tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "        return tokens\n",
    "\n",
    "    def train(self, text):\n",
    "        \"\"\"Trains the model by computing unigram and bigram counts.\"\"\"\n",
    "        tokens = self.preprocess_text(text)\n",
    "        self.vocab_size = len(set(tokens))  # Vocabulary size\n",
    "        self.unigram_counts.update(tokens)\n",
    "        self.bigram_counts.update(bigrams(tokens))\n",
    "\n",
    "    def compute_bigram_probability(self, word1, word2):\n",
    "        \"\"\"Computes bigram probability using Lidstone smoothing.\"\"\"\n",
    "        bigram_count = self.bigram_counts[(word1, word2)]\n",
    "        unigram_count = self.unigram_counts[word1]\n",
    "\n",
    "        probability = (bigram_count + self.lambda_smoothing) / (unigram_count + self.lambda_smoothing * self.vocab_size)\n",
    "        return probability\n",
    "\n",
    "    def generate_sequence(self, start_word, length=20):\n",
    "      \"\"\"Generates a sequence of words starting from a given word, with randomness.\"\"\"\n",
    "      sequence = [start_word]\n",
    "      for _ in range(length - 1):\n",
    "          # Get possible next words\n",
    "          possible_words = [word for word in self.unigram_counts.keys() if (sequence[-1], word) in self.bigram_counts]\n",
    "          if not possible_words:\n",
    "            # Backoff to unigram probabilities\n",
    "            possible_words = list(self.unigram_counts.keys())\n",
    "            if not possible_words:\n",
    "              break  # Stop if no valid next word\n",
    "            total_unigram_count = sum(self.unigram_counts.values())  # Sum of all word counts\n",
    "            denominator = total_unigram_count + (self.lambda_smoothing * len(self.unigram_counts))  # Apply Lidstone smoothing to the denominator\n",
    "            probabilities = [(self.unigram_counts[word] + self.lambda_smoothing) / denominator for word in possible_words]\n",
    "          else:\n",
    "            # Use bigram probabilities\n",
    "            probabilities = [self.compute_bigram_probability(sequence[-1], word) for word in possible_words]\n",
    "          # Compute probabilities for the next word using Lidstone smoothing\n",
    "          probabilities = [self.compute_bigram_probability(sequence[-1], word) for word in possible_words]\n",
    "          # Normalize probabilities to sum to 1\n",
    "          total_prob = sum(probabilities)\n",
    "          normalized_probs = [p / total_prob for p in probabilities]\n",
    "          # Choose the next word randomly based on the probability distribution\n",
    "          next_word = random.choices(possible_words, weights=normalized_probs, k=1)[0]\n",
    "          sequence.append(next_word)\n",
    "\n",
    "      return ' '.join(sequence)\n",
    "\n",
    "    def compute_perplexity(self, text):\n",
    "        \"\"\"Computes the perplexity of the model on a given text.\"\"\"\n",
    "        tokens = self.preprocess_text(text)\n",
    "        log_prob_sum = 0  # Sum of log probabilities\n",
    "        N = len(tokens) - 1  # Number of bigrams\n",
    "\n",
    "        for i in range(1, len(tokens)):\n",
    "            word1, word2 = tokens[i - 1], tokens[i]\n",
    "            prob = self.compute_bigram_probability(word1, word2)  # Compute P(word2 | word1)\n",
    "            log_prob_sum += math.log(prob)  # Add log probability to the sum\n",
    "\n",
    "        # Perplexity formula\n",
    "        perplexity = math.exp(-log_prob_sum / N)\n",
    "        return perplexity\n",
    "def read_local_txt(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "file_path = \"WarrenBuffet.txt\"  # Replace with your local file path\n",
    "text = read_local_txt(file_path)\n",
    "# Train the model\n",
    "bigram_model = BigramLanguageModel(lambda_smoothing=0.1)\n",
    "bigram_model.train(text)\n",
    "\n",
    "# Query probability\n",
    "word1, word2 = \"dog\", \"is\"\n",
    "prob = bigram_model.compute_bigram_probability(word1, word2)\n",
    "print(f\"P({word2} | {word1}) = {prob:.8f}\")\n",
    "\n",
    "# Generate sentence\n",
    "generated_sentence = bigram_model.generate_sequence(\"we\", length=100)\n",
    "print(\"Generated Sentence:\", generated_sentence)\n",
    " # Compute perplexity on the training data\n",
    "perplexity = bigram_model.compute_perplexity(text)\n",
    "print(f\"Perplexity on training data: {perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
